{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e0cae-b9bf-45eb-8c69-32989fed93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1(Ans) Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.\n",
    "\n",
    "Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, \n",
    "called ridge estimator, that, albeit biased, has lower variance than the OLS estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c756e88-683d-46fa-ab0d-96951615de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2(Ans)The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3ebc2-2abf-4fcc-a6e1-8853cdfb5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3(Ans) We iterate certain values onto the lambda and evaluate the model with a measurement such as 'Mean Square Error (MSE)'. So, the lambda value that minimizes MSE should be selected as the final mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87d301-04bd-48c8-a827-34fefdf0f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4(Ans) It essentially penalizes the least squares loss by applying a ridge penalty on the regression coefficients. The ridge penalty shrinks the regression coefficient estimate towards zero, \n",
    "but not exactly zero. For this reason, the ridge regression has long been criticized of not being able to perform variable selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e386d-4cd4-441e-a4de-c9f0f70b79fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5(Ans) Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity, The particular kind used by ridge regression is known as L2 regularization . \n",
    "In ridge regression, the penalty is the sum of the squares of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f601c17-5110-4d79-96da-3ed8d12b1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6(Ans) YES, Ridge Regression handle both categorical and continuous independent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c643ad-4cb6-41f2-bf1a-c32622d698fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7(Ans)The ridge coefficients are a reduced factor of the simple linear regression coefficients and thus never attain zero values but very small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b002d-bc39-4e3f-9a56-984e9164ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8(Ans) The ridge regression technique can be used to predict time-series. Ridge regression (RR) can also solve the multicollinearity problem that exists in linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
